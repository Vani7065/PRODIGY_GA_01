PRODIGY_GA_01 – Text Generation with GPT-2

🎯 Task Objective:

Train a model to generate coherent and contextually relevant text based on a given prompt using GPT-2, a transformer model by OpenAI.

🛠️ Technologies Used:

Python

GPT-2 (via Hugging Face Transformers)

Google Colab / Jupyter Notebook

📚 What I Learned:

Understanding transformer architecture

Fine-tuning GPT-2 on custom datasets

Text preprocessing and cleaning

Generating stylistically relevant text outputs

🔗 GitHub Repo:

https://github.com/Vani7065/PRODIGY_GA_01
